week 0
>math refresher


week 1
>knn
    k-nn algorithm
    train-validation-test split
    measure classification performance
    distance metrics
    curse of dimensionality

>decision trees
    interpretation and building of decision trees
    impurity functions / splitting heuristics
    overfitting
    good data science
    ensembles


week 2
>probabilistic inference
    maximum likelihood
    maximum a posteriori
    fully bayesian analysis
    prior, posterior, likelihood
    the iid assumption
    conjugate prior

    monotonic transforms for optimizization
    solving integrals by reverse engineering densities

week 3
>linear/ridge regressions
    optimizization-based approaches to regression have probabilistic interpretations
        least squares regression = maximum likelihood
        ridge regression = maxiumum a posteriori
    even nonlinear dependencies in the data can be captured by a model linear wrt weights w
    penalizing large weights helps to reduce overfitting
    full bayesian gives us data-dependent uncertainty estimates
